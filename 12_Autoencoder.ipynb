{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/binhvd/Data-Analytics-3-Labs/blob/main/12_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKWxdKD2APXh"
      },
      "source": [
        "# Training convolutional variational autoencoder on MNIST\n",
        "\n",
        "\n",
        "We would like to train a variational autoencoder based on [Kingma, Diederik P., and Max Welling. \"Auto-encoding variational bayes.\"](https://arxiv.org/abs/1312.6114) paper (and CNNs) on MNIST, and then study the properties of the inner representation. We would also like to do transfer learning to see, if the representations are useful for solving the classification task of notMNIST.\n",
        "\n",
        "Original of the first part of the task comes from [here](https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder_deconv.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WODv0XKTAPXj"
      },
      "source": [
        "## Preparations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrAf2U4qATSy"
      },
      "outputs": [],
      "source": [
        "!apt install graphviz\n",
        "!pip install pydot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m6Bi8wdAPXl"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Lambda\n",
        "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.losses import mse, binary_crossentropy\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ0YXVdkAPXs"
      },
      "outputs": [],
      "source": [
        "# Just a function for fancy plotting based on matplotlib\n",
        "# Nothing to do here.\n",
        "\n",
        "def plot_results(models,\n",
        "                 data,\n",
        "                 batch_size=128):\n",
        "    \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n",
        "\n",
        "    # Arguments:\n",
        "        models (tuple): encoder and decoder models\n",
        "        data (tuple): test data and label\n",
        "        batch_size (int): prediction batch size\n",
        "    \"\"\"\n",
        "\n",
        "    encoder, decoder = models\n",
        "    x_test, y_test = data\n",
        "\n",
        "    # display a 2D plot of the digit classes in the latent space\n",
        "    z_mean, _, _ = encoder.predict(x_test,\n",
        "                                   batch_size=batch_size)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.show()\n",
        "\n",
        "   \n",
        "    # display a 30x30 2D manifold of digits\n",
        "    n = 30\n",
        "    digit_size = 28\n",
        "    figure = np.zeros((digit_size * n, digit_size * n))\n",
        "    # linearly spaced coordinates corresponding to the 2D plot\n",
        "    # of digit classes in the latent space\n",
        "    grid_x = np.linspace(-4, 4, n)\n",
        "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
        "\n",
        "    for i, yi in enumerate(grid_y):\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            z_sample = np.array([[xi, yi]])\n",
        "            x_decoded = decoder.predict(z_sample)\n",
        "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "            figure[i * digit_size: (i + 1) * digit_size,\n",
        "                   j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    start_range = digit_size // 2\n",
        "    end_range = n * digit_size + start_range + 1\n",
        "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.imshow(figure, cmap='Greys_r')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyZtNqg58Z2g"
      },
      "source": [
        "## Loading MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-10-03T08:33:28.201476Z",
          "start_time": "2018-10-03T08:33:28.120150Z"
        },
        "id": "0382ECVrAPXw"
      },
      "outputs": [],
      "source": [
        "# MNIST dataset loading\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshaping to become \"tensors\" instead of matrix\n",
        "image_size = x_train.shape[1]\n",
        "x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
        "x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
        "\n",
        "# \"Normalizing\" by greyscale\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "print(\"X_train.shape:\",x_train.shape)\n",
        "print(\"X_test.shape:\",x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbRpJBBmAPX0"
      },
      "source": [
        "## Building the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4Zxu5p_APX0"
      },
      "outputs": [],
      "source": [
        "# network parameters\n",
        "input_shape = (image_size, image_size, 1)\n",
        "batch_size = 128\n",
        "kernel_size = 3\n",
        "filters = 16\n",
        "strides = 2\n",
        "padding = 'same'\n",
        "latent_dim = 2\n",
        "epochs = 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9AoutWOAPX3"
      },
      "source": [
        "### Task: Read and understand the \"The main trick\"\n",
        "\n",
        "The \"heart\" of all variational autoencoder methods lies the \"reparametrization trick\", which is realized by the following function.\n",
        "\n",
        "Please study the mechanism below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvGiMpJuAPX5"
      },
      "outputs": [],
      "source": [
        "# Reparameterization trick\n",
        "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
        "# then z = z_mean + sqrt(var)*eps\n",
        "#\n",
        "# Reference\n",
        "# Kingma, Diederik P., and Max Welling.\n",
        "# \"Auto-encoding variational bayes.\"\n",
        "# https://arxiv.org/abs/1312.6114\n",
        "def sampling(args):\n",
        "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
        "\n",
        "    # Arguments:\n",
        "        args (tensor): mean and log of variance of Q(z|X)\n",
        "\n",
        "    # Returns:\n",
        "        z (tensor): sampled latent vector\n",
        "    \"\"\"\n",
        "\n",
        "    z_mean, z_log_var = args\n",
        "\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    # by default, random_normal has mean=0 and std=1.0\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mufPaUL4APX8"
      },
      "source": [
        "### Building the encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7TFRgKVAPX9"
      },
      "outputs": [],
      "source": [
        "# build encoder model\n",
        "inputs = Input(shape=input_shape, name='encoder_input')\n",
        "x = inputs\n",
        "for i in range(2):\n",
        "    filters *= 2\n",
        "    x = Conv2D(filters=filters,\n",
        "               kernel_size=kernel_size,\n",
        "               activation='relu',\n",
        "               strides=strides,\n",
        "               padding=padding)(x)\n",
        "\n",
        "# shape info needed to build decoder model\n",
        "shape = K.int_shape(x)\n",
        "\n",
        "# generate latent vector Q(z|X)\n",
        "x = Flatten()(x)\n",
        "\n",
        "# ------- TASK ----------\n",
        "\n",
        "# Please define a \"fully connected\" neural layer with the ReLU activation and 16 units\n",
        "# The input should be x.\n",
        "# Please try to figure out the output variable based on the next line - easiest solution.\n",
        "# Please don't forget that there is a separate () for defining the layer and for calling it on a value!\n",
        "# This layer does not need a name.\n",
        "# The import for this is already handled ;-)\n",
        "\n",
        "...\n",
        "\n",
        "# These lines help to figure out the variable name before.\n",
        "# Please observe the fact, that the processing \"branches\" here, \n",
        "# so there are _TWO_ things being calculated here in parallel\n",
        "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "\n",
        "\n",
        "# ------- TASK ----------\n",
        "#\n",
        "# Use reparameterization trick to push the sampling out as output of the encoder\n",
        "#\n",
        "# Use Keras.layers.Lambda - which can execute the `sampling` function defined above in the prior cell  \n",
        "# Lambda is already imported in the default namespace (see beginning of notebook)\n",
        "# Input for the layer is the _list_ of prior two z_... outputs\n",
        "# Please ensure, that the name of the sampling output is simply z \n",
        "# note that \"output_shape\" isn't necessary with the TensorFlow backend, \n",
        "# but for good riddance you can define it as a _tuple_ with only the latent dimension inside\n",
        "z = ...\n",
        "\n",
        "# instantiate encoder model\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "# We need z_mean, z_log_var for computing KL divergence later on, so we get this out of the model also,\n",
        "# not just the sampled z itself.\n",
        "\n",
        "encoder.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS2Ar5aiAPYA"
      },
      "source": [
        "### Building the decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJsiTR3MAPYB"
      },
      "outputs": [],
      "source": [
        "# build decoder model\n",
        "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
        "x = Dense(shape[1] * shape[2] * shape[3], activation='relu')(latent_inputs)\n",
        "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
        "\n",
        "\n",
        "for i in range(2):\n",
        "\n",
        "    # ------- TASK ----------\n",
        "    # \n",
        "    # Define the deconvolution layers for the decoder model.\n",
        "    # The design pattrn comes from the encoder.\n",
        "    # Use the \"transposed\" layer \n",
        "    # (notice, it a a layer with separate name, already imported into the namespace !)\n",
        "    # _ALL_ other parameters of the layer should conform to the encoder.\n",
        "    # Continue to use functional style, pass on the \"same\" variable\n",
        "    # _OBSERVE INDENTATION_ please!\n",
        "    \n",
        "    ...\n",
        "    \n",
        "    filters //= 2\n",
        "    \n",
        "    \n",
        "    \n",
        "# ------- TASK ----------\n",
        "#\n",
        "# Define a final deconvolution layer (get inspiration from the task one above) \n",
        "# with _sigmoid_ activation for final output.\n",
        "# filters should be 1, kernel size the one we used above, padding same, \n",
        "# name of the layer should be \"decoder_output\"\n",
        "# Use functional style.\n",
        "outputs = ...\n",
        "\n",
        "# instantiate decoder model\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jVqHNz5APYF"
      },
      "source": [
        "### Task: The VAE itself\n",
        "\n",
        "TASK: Observe the buildup of the composite loss function!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDaFajuzAPYF"
      },
      "outputs": [],
      "source": [
        "# instantiate VAE model\n",
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, outputs, name='vae')\n",
        "\n",
        "# Building the composite loss\n",
        "# VAE loss = mse_loss + kl_loss \n",
        "reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n",
        "reconstruction_loss *= image_size * image_size\n",
        "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "vae.add_loss(vae_loss)\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "vae.compile(optimizer=optimizer)\n",
        "vae.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXK9cDgqAPYJ"
      },
      "source": [
        "## Training and results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-IdjM4DAPYK"
      },
      "outputs": [],
      "source": [
        "# ------- TASK ----------\n",
        "#\n",
        "# Fit the vae model on x_train with epochs, batch_size defined at the beginning of the notebook\n",
        "# Use validation data, a _tuple_ of x_test and None, since we have no y\n",
        "# for syntax consult https://keras.io/models/model/#fit\n",
        "\n",
        "...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrBSi9wk8Z2i"
      },
      "source": [
        "## Task: Observe the result\n",
        "\n",
        "TASK: Please observe the resulting latent space, let's discuss!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAponrlhAPYN"
      },
      "outputs": [],
      "source": [
        "models = (encoder, decoder)\n",
        "data = (x_test, y_test)\n",
        "\n",
        "plot_results(models, data, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ6xr3cn8Z2i"
      },
      "source": [
        "### Task: Play around with the representation!\n",
        "\n",
        "TASK: give in two coordinates as data for the decoder, and observe the result!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXIdwGd9APYQ"
      },
      "outputs": [],
      "source": [
        "data = [...]\n",
        "\n",
        "\n",
        "z_sample = np.array([data])\n",
        "x_decoded = decoder.predict(z_sample)\n",
        "\n",
        "plt.imshow(x_decoded.reshape(image_size,image_size))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ILoOlbR8Z2j"
      },
      "source": [
        "### We save the encoder model for transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97f9AaiQAPYS"
      },
      "outputs": [],
      "source": [
        "encoder.save(\"encoder.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK4FcgvOPfab"
      },
      "source": [
        "# Transfer learning on notMNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2P-l2ZqPm0N"
      },
      "source": [
        "## Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhvPB361APYX"
      },
      "outputs": [],
      "source": [
        "loaded_encoder = load_model(\"encoder.h5\")\n",
        "\n",
        "# ---- Optional task AFTER the notebook is finished ---\n",
        "# You canget back to here...\n",
        "#\n",
        "# This would keep the encoder static. Worth a try what it does :-)\n",
        "#for layer in loaded_encoder.layers:\n",
        "#        layer.trainable = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2NoxB2PPqxx"
      },
      "source": [
        "## Loading and normalizing the data\n",
        "\n",
        "No task here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7VEuaIpGWac"
      },
      "outputs": [],
      "source": [
        "! wget https://github.com/davidflanagan/notMNIST-to-MNIST/raw/master/t10k-images-idx3-ubyte.gz\n",
        "! wget https://github.com/davidflanagan/notMNIST-to-MNIST/raw/master/t10k-labels-idx1-ubyte.gz\n",
        "! wget https://github.com/davidflanagan/notMNIST-to-MNIST/raw/master/train-images-idx3-ubyte.gz\n",
        "! wget https://github.com/davidflanagan/notMNIST-to-MNIST/raw/master/train-labels-idx1-ubyte.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpKfPT6fGXb4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gzip\n",
        "\n",
        "def load_NOTmnist(path, kind='train'):\n",
        "    \"\"\"Load MNIST data from `path`.\n",
        "    \"\"\"\n",
        "    labels_path = os.path.join(path,'%s-labels-idx1-ubyte.gz' % kind)\n",
        "    images_path = os.path.join(path,'%s-images-idx3-ubyte.gz' % kind)\n",
        "\n",
        "    with gzip.open(labels_path, 'rb') as lbpath:\n",
        "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
        "                               offset=8)\n",
        "\n",
        "    with gzip.open(images_path, 'rb') as imgpath:\n",
        "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
        "                               offset=16).reshape(len(labels), 784)\n",
        "\n",
        "    return images, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFD3ZQovGmbY"
      },
      "outputs": [],
      "source": [
        "train_images, train_labels = load_NOTmnist('.')\n",
        "valid_images, valid_labels = load_NOTmnist('.', 't10k')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de-okC6_APYc"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.layers import concatenate, Dropout\n",
        "\n",
        "\n",
        "classes = 10\n",
        "\n",
        "\n",
        "train_labels = train_labels.astype('int32')\n",
        "train_labels = to_categorical(train_labels, 10)\n",
        "\n",
        "train_images = train_images.astype('float32') / 255.\n",
        "train_images = train_images.reshape((-1, image_size, image_size, 1))\n",
        "\n",
        "valid_labels = valid_labels.astype('int32')\n",
        "valid_labels = to_categorical(valid_labels, 10)\n",
        "\n",
        "valid_images = valid_images.astype('float32') / 255.\n",
        "valid_images = valid_images.reshape((-1, image_size, image_size, 1))\n",
        "\n",
        "\n",
        "print(\"Loaded data X shape:\", train_images.shape)\n",
        "print(\"Loaded data Y shape:\", train_labels.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rGNEp2kPuVM"
      },
      "source": [
        "## Task: Defining the model\n",
        "\n",
        "TASK: Observe the definition of the model, especially:\n",
        "- the call for the loaded model\n",
        "- the concatenation of the encoder output\n",
        "- the final softmax layer\n",
        "- the metrics we use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PxZ-GjAAPYg"
      },
      "outputs": [],
      "source": [
        "inputs = Input(shape=input_shape, name='encoder_input')\n",
        "\n",
        "latent = loaded_encoder(inputs) #trainable=False?? You can get back to this LATER in the optional task above...\n",
        "\n",
        "concat = concatenate(latent[0:2])\n",
        "\n",
        "x = Dense(100, activation=\"relu\")(concat)\n",
        "\n",
        "predictions = Dense(10, activation='softmax')(x)\n",
        "\n",
        "discriminator = Model(inputs, predictions, name='discriminator')\n",
        "\n",
        "discriminator.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "discriminator.summary()\n",
        "\n",
        "plot_model(discriminator, show_shapes=True)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tIby0L9P1mw"
      },
      "source": [
        "## Task: Training the model\n",
        "\n",
        "TASK: Observe how the training progresses! Share your observations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7bUTR2ABGZz"
      },
      "outputs": [],
      "source": [
        "discriminator.fit(x=train_images, y=train_labels, batch_size=128, epochs=5, validation_data=(valid_images, valid_labels))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Day10_Task_VAE.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}